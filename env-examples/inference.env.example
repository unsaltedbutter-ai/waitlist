# ==================================================
# inference.env.example
# Copy to ~/.unsaltedbutter/inference.env
# Runs on: Mac Studio (standalone, no shared.env needed)
# ==================================================

# Server
INFERENCE_HOST=0.0.0.0
INFERENCE_PORT=8420
LOG_LEVEL=INFO

# Model backend: mock, llama_cpp, mlx, or openai
MODEL_BACKEND=mock

# Path to the GGUF model file (for llama_cpp backend)
# or HuggingFace model ID / local path (for mlx backend)
MODEL_PATH=~/models/qwen3-vl-32b-q4_k_m.gguf

# Model parameters
CONTEXT_LENGTH=8192
MAX_TOKENS=1024
TEMPERATURE=0.1
GPU_LAYERS=-1

# Image preprocessing
MAX_IMAGE_DIMENSION=2560

# Safety: refuse prompts asking to read password contents
PASSWORD_GUARD_ENABLED=true

# OpenAI-compatible VLM endpoint (MODEL_BACKEND=openai)
# VLM_BASE_URL=https://api.ppq.ai
# VLM_API_KEY=your-ppq-api-key-here
# VLM_MODEL=qwen3-vl-32b-instruct
# For local auto-start (VLM_BASE_URL=http://localhost:8080):
#   MODEL_PATH and GPU_LAYERS are used to launch llama-cpp-python server
