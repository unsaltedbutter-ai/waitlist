# Inference server configuration (Mac Studio M3 Ultra)

# Server
INFERENCE_HOST=0.0.0.0
INFERENCE_PORT=8420
LOG_LEVEL=INFO

# Model backend: mock, llama_cpp, or mlx
MODEL_BACKEND=mock

# Path to the GGUF model file (for llama_cpp backend)
# or HuggingFace model ID / local path (for mlx backend)
MODEL_PATH=~/models/qwen3-vl-32b-q4_k_m.gguf

# Model parameters
CONTEXT_LENGTH=8192
MAX_TOKENS=1024
TEMPERATURE=0.1
GPU_LAYERS=-1

# Image preprocessing
MAX_IMAGE_DIMENSION=2560

# Safety: refuse prompts asking to read password contents
PASSWORD_GUARD_ENABLED=true
