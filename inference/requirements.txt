# Inference server requirements (runs on Mac Studio M3 Ultra)

# Core server
fastapi>=0.115.0
uvicorn[standard]>=0.32.0
pydantic>=2.0.0
Pillow>=10.0.0
python-dotenv>=1.0.0

# Testing
pytest>=8.0.0
httpx>=0.27.0           # for FastAPI TestClient

# Backend: llama.cpp (uncomment when hardware is ready)
# llama-cpp-python>=0.3.0

# Backend: MLX (uncomment for Apple Silicon native)
# mlx-vlm>=0.1.0
